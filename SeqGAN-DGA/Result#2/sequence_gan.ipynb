{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "from dataloader import Gen_Data_loader, Dis_dataloader\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from rollout import ROLLOUT\n",
    "#from target_lstm import TARGET_LSTM\n",
    "#import domain_translate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Generator  Hyper-parameters\n",
    "######################################################################################\n",
    "EMB_DIM = 32 # embedding dimension\n",
    "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
    "SEQ_LENGTH = 32 # sequence length\n",
    "START_TOKEN = 0\n",
    "PRE_EPOCH_NUM = 50 # supervise (maximum likelihood estimation) epochs (預設為120)\n",
    "SEED = 88\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Discriminator  Hyper-parameters\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "PRETRAIN_DIS_NUM = 50 #pre-train discriminator times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Basic Training Parameters\n",
    "#########################################################################################\n",
    "TOTAL_BATCH = 144\n",
    "positive_file = ('../Dataset/AlexaTop100K_Separated_Digital/'\n",
    "                  +'TopDomainName.Less33.Separated.Digital-ALL-OF-100000.txt') # 預設值為 'save/real_data.txt'\n",
    "negative_file = 'save/' #預設值為 'save/generator_sample.txt' 更改為會更變\n",
    "eval_file = 'save/eval_file.txt'\n",
    "generated_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n",
    "    Digit2Real = False\n",
    "    # Generate Samples\n",
    "    generated_samples = []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        generated_samples.extend(trainable_model.generate(sess))\n",
    "\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for poem in generated_samples:\n",
    "            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n",
    "            fout.write(buffer)\n",
    "            \n",
    "    if \"adversarial_gen\" in output_file:\n",
    "        now=output_file[37:]\n",
    "        output_file=\"save/adversarial_gen/generator_fake_domain_name_\"+now\n",
    "        Digit2Real = True\n",
    "    \n",
    "    if Digit2Real == True :\n",
    "        digital_table = [\" \",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\".\",\"-\",\n",
    "                            \"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\n",
    "                            \"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\"_\"]    \n",
    "        with open(output_file, 'w') as fout:\n",
    "            for poem in generated_samples:\n",
    "                buffer = \"\".join([digital_table[int(x)] for x in poem]) + '\\n'\n",
    "                fout.write(buffer)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_loss(sess, target_lstm, data_loader):\n",
    "    # target_loss means the oracle negative log-likelihood tested with the oracle model \"target_lstm\"\n",
    "    # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473\n",
    "    nll = []\n",
    "    data_loader.reset_pointer()\n",
    "\n",
    "    for it in range(data_loader.num_batch):\n",
    "        batch = data_loader.next_batch()\n",
    "        #print(target_lstm.pretrain_loss)\n",
    "        #print({target_lstm.x: batch})\n",
    "        g_loss = sess.run(target_lstm.pretrain_loss, {target_lstm.x: batch})\n",
    "        nll.append(g_loss)\n",
    "\n",
    "    return np.mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_train_epoch(sess, trainable_model, data_loader):\n",
    "    # Pre-train the generator using MLE for one epoch\n",
    "    supervised_g_losses = []\n",
    "    data_loader.reset_pointer()\n",
    "    \n",
    "    for it in range(data_loader.num_batch):\n",
    "    #for it in range(50):\n",
    "        #print(\"pre_train\"+str(it))\n",
    "        if it % int(data_loader.num_batch / 30) == 0:\n",
    "            print(\"pre_train_iteration : {0:6} / {1:6}\".format( (it+1), (data_loader.num_batch) ), end=\"\\r\")\n",
    "        batch = data_loader.next_batch()\n",
    "        _, g_loss = trainable_model.pretrain_step(sess, batch)\n",
    "        supervised_g_losses.append(g_loss)\n",
    "\n",
    "    return np.mean(supervised_g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "assert START_TOKEN == 0\n",
    "\n",
    "gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n",
    "likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n",
    "vocab_size = 40 #預設5000 \n",
    "dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
    "\n",
    "generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n",
    "#target_params = pickle.load(open('save/target_params_py3.pkl','rb'))\n",
    "#target_lstm = TARGET_LSTM(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, target_params) # The oracle model\n",
    "\n",
    "discriminator = Discriminator(sequence_length=32, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
    "                            filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 參數配置\n",
    "config = tf.ConfigProto()\n",
    "# 使用allow_growth option，剛一開始分配少量的GPU容量，然後按需慢慢的增加\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pre-training...\n",
      "[MLE Epoch]:     1 [Cost Time]:      76.43 secs [ETA]:    3745.11 secs\n",
      "[MLE Epoch]:     2 [Cost Time]:     145.52 secs [ETA]:    3492.39 secs\n",
      "[MLE Epoch]:     3 [Cost Time]:     214.72 secs [ETA]:    3363.91 secs\n",
      "[MLE Epoch]:     4 [Cost Time]:     284.24 secs [ETA]:    3268.75 secs\n",
      "[MLE Epoch]:     5 [Cost Time]:     357.68 secs [ETA]:    3219.08 secs\n",
      "[MLE Epoch]:     6 [Cost Time]:     433.32 secs [ETA]:    3177.70 secs\n",
      "[MLE Epoch]:     7 [Cost Time]:     502.47 secs [ETA]:    3086.63 secs\n",
      "[MLE Epoch]:     8 [Cost Time]:     573.53 secs [ETA]:    3011.03 secs\n",
      "[MLE Epoch]:     9 [Cost Time]:     646.54 secs [ETA]:    2945.36 secs\n",
      "[MLE Epoch]:    10 [Cost Time]:     717.55 secs [ETA]:    2870.22 secs\n",
      "[MLE Epoch]:    11 [Cost Time]:     795.32 secs [ETA]:    2819.78 secs\n",
      "[MLE Epoch]:    12 [Cost Time]:     868.89 secs [ETA]:    2751.50 secs\n",
      "[MLE Epoch]:    13 [Cost Time]:     938.43 secs [ETA]:    2670.92 secs\n",
      "[MLE Epoch]:    14 [Cost Time]:    1009.84 secs [ETA]:    2596.73 secs\n",
      "[MLE Epoch]:    15 [Cost Time]:    1081.50 secs [ETA]:    2523.51 secs\n",
      "[MLE Epoch]:    16 [Cost Time]:    1156.39 secs [ETA]:    2457.33 secs\n",
      "[MLE Epoch]:    17 [Cost Time]:    1229.01 secs [ETA]:    2385.73 secs\n",
      "[MLE Epoch]:    18 [Cost Time]:    1298.52 secs [ETA]:    2308.48 secs\n",
      "[MLE Epoch]:    19 [Cost Time]:    1370.19 secs [ETA]:    2235.57 secs\n",
      "[MLE Epoch]:    20 [Cost Time]:    1440.24 secs [ETA]:    2160.37 secs\n",
      "[MLE Epoch]:    21 [Cost Time]:    1518.59 secs [ETA]:    2097.10 secs\n",
      "[MLE Epoch]:    22 [Cost Time]:    1588.11 secs [ETA]:    2021.23 secs\n",
      "[MLE Epoch]:    23 [Cost Time]:    1660.17 secs [ETA]:    1948.90 secs\n",
      "[MLE Epoch]:    24 [Cost Time]:    1729.74 secs [ETA]:    1873.88 secs\n",
      "[MLE Epoch]:    25 [Cost Time]:    1800.61 secs [ETA]:    1800.61 secs\n",
      "[MLE Epoch]:    26 [Cost Time]:    1877.61 secs [ETA]:    1733.18 secs\n",
      "[MLE Epoch]:    27 [Cost Time]:    1948.83 secs [ETA]:    1660.11 secs\n",
      "[MLE Epoch]:    28 [Cost Time]:    2018.93 secs [ETA]:    1586.30 secs\n",
      "[MLE Epoch]:    29 [Cost Time]:    2090.01 secs [ETA]:    1513.45 secs\n",
      "[MLE Epoch]:    30 [Cost Time]:    2161.65 secs [ETA]:    1441.10 secs\n",
      "[MLE Epoch]:    31 [Cost Time]:    2236.08 secs [ETA]:    1370.50 secs\n",
      "[MLE Epoch]:    32 [Cost Time]:    2306.08 secs [ETA]:    1297.17 secs\n",
      "[MLE Epoch]:    33 [Cost Time]:    2374.78 secs [ETA]:    1223.37 secs\n",
      "[MLE Epoch]:    34 [Cost Time]:    2446.57 secs [ETA]:    1151.33 secs\n",
      "[MLE Epoch]:    35 [Cost Time]:    2516.01 secs [ETA]:    1078.29 secs\n",
      "[MLE Epoch]:    36 [Cost Time]:    2593.38 secs [ETA]:    1008.54 secs\n",
      "[MLE Epoch]:    37 [Cost Time]:    2663.33 secs [ETA]:     935.76 secs\n",
      "[MLE Epoch]:    38 [Cost Time]:    2734.85 secs [ETA]:     863.64 secs\n",
      "[MLE Epoch]:    39 [Cost Time]:    2805.50 secs [ETA]:     791.29 secs\n",
      "[MLE Epoch]:    40 [Cost Time]:    2875.19 secs [ETA]:     718.80 secs\n",
      "[MLE Epoch]:    41 [Cost Time]:    2949.68 secs [ETA]:     647.49 secs\n",
      "[MLE Epoch]:    42 [Cost Time]:    3019.22 secs [ETA]:     575.09 secs\n",
      "[MLE Epoch]:    43 [Cost Time]:    3088.34 secs [ETA]:     502.75 secs\n",
      "[MLE Epoch]:    44 [Cost Time]:    3157.38 secs [ETA]:     430.55 secs\n",
      "[MLE Epoch]:    45 [Cost Time]:    3230.59 secs [ETA]:     358.95 secs\n",
      "[MLE Epoch]:    46 [Cost Time]:    3305.30 secs [ETA]:     287.42 secs\n",
      "[MLE Epoch]:    47 [Cost Time]:    3374.79 secs [ETA]:     215.41 secs\n",
      "[MLE Epoch]:    48 [Cost Time]:    3443.90 secs [ETA]:     143.50 secs\n",
      "[MLE Epoch]:    49 [Cost Time]:    3513.37 secs [ETA]:      71.70 secs\n",
      "[MLE Epoch]:    50 [Cost Time]:    3582.84 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "# First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n",
    "#generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file)\n",
    "gen_data_loader.create_batches(positive_file)\n",
    "\n",
    "log = open('save/experiment-log.txt', 'w')\n",
    "#  pre-train generator\n",
    "print('Start pre-training...')\n",
    "log.write('pre-training...\\n')\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "for epoch in range(PRE_EPOCH_NUM):\n",
    "    loss = pre_train_epoch(sess, generator, gen_data_loader)    \n",
    "    if epoch % 5 == 0:\n",
    "        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
    "        likelihood_data_loader.create_batches(eval_file)\n",
    "        '''test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
    "        print('pre-train epoch ', epoch, 'test_loss ', test_loss)\n",
    "        buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "        log.write(buffer)'''\n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (epoch+1) )*(PRE_EPOCH_NUM-(epoch+1))\n",
    "    print(\"[MLE Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (epoch+1), after_time, eta_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pre-training discriminator...\n",
      "[Pre-train Discriminator Epoch]:     1 [Cost Time]:      35.48 secs [ETA]:    1738.46 secs\n",
      "[Pre-train Discriminator Epoch]:     2 [Cost Time]:      66.87 secs [ETA]:    1604.92 secs\n",
      "[Pre-train Discriminator Epoch]:     3 [Cost Time]:      97.67 secs [ETA]:    1530.22 secs\n",
      "[Pre-train Discriminator Epoch]:     4 [Cost Time]:     128.55 secs [ETA]:    1478.35 secs\n",
      "[Pre-train Discriminator Epoch]:     5 [Cost Time]:     159.27 secs [ETA]:    1433.39 secs\n",
      "[Pre-train Discriminator Epoch]:     6 [Cost Time]:     190.16 secs [ETA]:    1394.48 secs\n",
      "[Pre-train Discriminator Epoch]:     7 [Cost Time]:     221.35 secs [ETA]:    1359.74 secs\n",
      "[Pre-train Discriminator Epoch]:     8 [Cost Time]:     252.56 secs [ETA]:    1325.93 secs\n",
      "[Pre-train Discriminator Epoch]:     9 [Cost Time]:     283.84 secs [ETA]:    1293.04 secs\n",
      "[Pre-train Discriminator Epoch]:    10 [Cost Time]:     314.95 secs [ETA]:    1259.80 secs\n",
      "[Pre-train Discriminator Epoch]:    11 [Cost Time]:     346.14 secs [ETA]:    1227.22 secs\n",
      "[Pre-train Discriminator Epoch]:    12 [Cost Time]:     377.41 secs [ETA]:    1195.13 secs\n",
      "[Pre-train Discriminator Epoch]:    13 [Cost Time]:     408.55 secs [ETA]:    1162.81 secs\n",
      "[Pre-train Discriminator Epoch]:    14 [Cost Time]:     439.66 secs [ETA]:    1130.56 secs\n",
      "[Pre-train Discriminator Epoch]:    15 [Cost Time]:     470.85 secs [ETA]:    1098.66 secs\n",
      "[Pre-train Discriminator Epoch]:    16 [Cost Time]:     501.93 secs [ETA]:    1066.60 secs\n",
      "[Pre-train Discriminator Epoch]:    17 [Cost Time]:     533.08 secs [ETA]:    1034.80 secs\n",
      "[Pre-train Discriminator Epoch]:    18 [Cost Time]:     564.14 secs [ETA]:    1002.92 secs\n",
      "[Pre-train Discriminator Epoch]:    19 [Cost Time]:     595.16 secs [ETA]:     971.05 secs\n",
      "[Pre-train Discriminator Epoch]:    20 [Cost Time]:     626.14 secs [ETA]:     939.21 secs\n",
      "[Pre-train Discriminator Epoch]:    21 [Cost Time]:     657.36 secs [ETA]:     907.78 secs\n",
      "[Pre-train Discriminator Epoch]:    22 [Cost Time]:     688.46 secs [ETA]:     876.22 secs\n",
      "[Pre-train Discriminator Epoch]:    23 [Cost Time]:     718.85 secs [ETA]:     843.87 secs\n",
      "[Pre-train Discriminator Epoch]:    24 [Cost Time]:     750.49 secs [ETA]:     813.03 secs\n",
      "[Pre-train Discriminator Epoch]:    25 [Cost Time]:     781.57 secs [ETA]:     781.57 secs\n",
      "[Pre-train Discriminator Epoch]:    26 [Cost Time]:     812.80 secs [ETA]:     750.28 secs\n",
      "[Pre-train Discriminator Epoch]:    27 [Cost Time]:     843.92 secs [ETA]:     718.90 secs\n",
      "[Pre-train Discriminator Epoch]:    28 [Cost Time]:     875.05 secs [ETA]:     687.54 secs\n",
      "[Pre-train Discriminator Epoch]:    29 [Cost Time]:     906.15 secs [ETA]:     656.18 secs\n",
      "[Pre-train Discriminator Epoch]:    30 [Cost Time]:     937.48 secs [ETA]:     624.99 secs\n",
      "[Pre-train Discriminator Epoch]:    31 [Cost Time]:     968.17 secs [ETA]:     593.39 secs\n",
      "[Pre-train Discriminator Epoch]:    32 [Cost Time]:     999.48 secs [ETA]:     562.21 secs\n",
      "[Pre-train Discriminator Epoch]:    33 [Cost Time]:    1030.89 secs [ETA]:     531.06 secs\n",
      "[Pre-train Discriminator Epoch]:    34 [Cost Time]:    1062.03 secs [ETA]:     499.78 secs\n",
      "[Pre-train Discriminator Epoch]:    35 [Cost Time]:    1093.31 secs [ETA]:     468.56 secs\n",
      "[Pre-train Discriminator Epoch]:    36 [Cost Time]:    1124.46 secs [ETA]:     437.29 secs\n",
      "[Pre-train Discriminator Epoch]:    37 [Cost Time]:    1155.07 secs [ETA]:     405.84 secs\n",
      "[Pre-train Discriminator Epoch]:    38 [Cost Time]:    1186.43 secs [ETA]:     374.66 secs\n",
      "[Pre-train Discriminator Epoch]:    39 [Cost Time]:    1217.43 secs [ETA]:     343.38 secs\n",
      "[Pre-train Discriminator Epoch]:    40 [Cost Time]:    1248.50 secs [ETA]:     312.12 secs\n",
      "[Pre-train Discriminator Epoch]:    41 [Cost Time]:    1279.66 secs [ETA]:     280.90 secs\n",
      "[Pre-train Discriminator Epoch]:    42 [Cost Time]:    1310.16 secs [ETA]:     249.56 secs\n",
      "[Pre-train Discriminator Epoch]:    43 [Cost Time]:    1341.62 secs [ETA]:     218.40 secs\n",
      "[Pre-train Discriminator Epoch]:    44 [Cost Time]:    1373.12 secs [ETA]:     187.24 secs\n",
      "[Pre-train Discriminator Epoch]:    45 [Cost Time]:    1404.14 secs [ETA]:     156.02 secs\n",
      "[Pre-train Discriminator Epoch]:    46 [Cost Time]:    1435.39 secs [ETA]:     124.82 secs\n",
      "[Pre-train Discriminator Epoch]:    47 [Cost Time]:    1466.34 secs [ETA]:      93.60 secs\n",
      "[Pre-train Discriminator Epoch]:    48 [Cost Time]:    1497.28 secs [ETA]:      62.39 secs\n",
      "[Pre-train Discriminator Epoch]:    49 [Cost Time]:    1528.28 secs [ETA]:      31.19 secs\n",
      "[Pre-train Discriminator Epoch]:    50 [Cost Time]:    1559.85 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "print('Start pre-training discriminator...')\n",
    "# Train 3 epoch on the generated data and do this for 50 times\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "pre_D_epoch = 0\n",
    "for _ in range(PRETRAIN_DIS_NUM): #50\n",
    "    pretrain_D = ( negative_file + \"pretrain_discriminator.txt\")\n",
    "    generate_samples(sess, generator, BATCH_SIZE, generated_num, pretrain_D)\n",
    "    dis_data_loader.load_train_data(positive_file, pretrain_D)\n",
    "    for _ in range(1):\n",
    "        dis_data_loader.reset_pointer()\n",
    "        for it in range(dis_data_loader.num_batch):\n",
    "            if it % int(dis_data_loader.num_batch / 30) == 0:\n",
    "                print(\"pre_train_iteration : {0:6} / {1:6}\".format( (it+1), (dis_data_loader.num_batch) ), end=\"\\r\")\n",
    "            x_batch, y_batch = dis_data_loader.next_batch()\n",
    "            feed = {\n",
    "                discriminator.input_x: x_batch,\n",
    "                discriminator.input_y: y_batch,\n",
    "                discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "            }\n",
    "            _ = sess.run(discriminator.train_op, feed)\n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (pre_D_epoch+1) )*(PRETRAIN_DIS_NUM-(pre_D_epoch+1))\n",
    "    print(\"[Pre-train Discriminator Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (pre_D_epoch+1), after_time, eta_time))\n",
    "    pre_D_epoch = pre_D_epoch +1\n",
    "\n",
    "rollout = ROLLOUT(generator, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "Start Adversarial Training...\n",
      "[GAN Epoch]:     1 [Cost Time]:     104.23 secs [ETA]:   14905.39 secs\n",
      "[GAN Epoch]:     2 [Cost Time]:     209.35 secs [ETA]:   14864.03 secs\n",
      "[GAN Epoch]:     3 [Cost Time]:     313.31 secs [ETA]:   14725.52 secs\n",
      "[GAN Epoch]:     4 [Cost Time]:     416.63 secs [ETA]:   14581.90 secs\n",
      "[GAN Epoch]:     5 [Cost Time]:     520.37 secs [ETA]:   14466.38 secs\n",
      "[GAN Epoch]:     6 [Cost Time]:     624.98 secs [ETA]:   14374.56 secs\n",
      "[GAN Epoch]:     7 [Cost Time]:     730.34 secs [ETA]:   14293.88 secs\n",
      "[GAN Epoch]:     8 [Cost Time]:     834.60 secs [ETA]:   14188.24 secs\n",
      "[GAN Epoch]:     9 [Cost Time]:     940.18 secs [ETA]:   14102.69 secs\n",
      "[GAN Epoch]:    10 [Cost Time]:    1043.98 secs [ETA]:   13989.37 secs\n",
      "[GAN Epoch]:    11 [Cost Time]:    1147.53 secs [ETA]:   13874.73 secs\n",
      "[GAN Epoch]:    12 [Cost Time]:    1253.40 secs [ETA]:   13787.36 secs\n",
      "[GAN Epoch]:    13 [Cost Time]:    1356.14 secs [ETA]:   13665.77 secs\n",
      "[GAN Epoch]:    14 [Cost Time]:    1456.87 secs [ETA]:   13528.04 secs\n",
      "[GAN Epoch]:    15 [Cost Time]:    1558.25 secs [ETA]:   13400.98 secs\n",
      "[GAN Epoch]:    16 [Cost Time]:    1661.22 secs [ETA]:   13289.79 secs\n",
      "[GAN Epoch]:    17 [Cost Time]:    1764.02 secs [ETA]:   13178.29 secs\n",
      "[GAN Epoch]:    18 [Cost Time]:    1867.16 secs [ETA]:   13070.15 secs\n",
      "[GAN Epoch]:    19 [Cost Time]:    1967.41 secs [ETA]:   12943.46 secs\n",
      "[GAN Epoch]:    20 [Cost Time]:    2068.35 secs [ETA]:   12823.77 secs\n",
      "[GAN Epoch]:    21 [Cost Time]:    2169.57 secs [ETA]:   12707.46 secs\n",
      "[GAN Epoch]:    22 [Cost Time]:    2270.42 secs [ETA]:   12590.52 secs\n",
      "[GAN Epoch]:    23 [Cost Time]:    2371.32 secs [ETA]:   12475.23 secs\n",
      "[GAN Epoch]:    24 [Cost Time]:    2471.34 secs [ETA]:   12356.71 secs\n",
      "[GAN Epoch]:    25 [Cost Time]:    2572.63 secs [ETA]:   12245.71 secs\n",
      "[GAN Epoch]:    26 [Cost Time]:    2674.55 secs [ETA]:   12138.35 secs\n",
      "[GAN Epoch]:    27 [Cost Time]:    2777.47 secs [ETA]:   12035.71 secs\n",
      "[GAN Epoch]:    28 [Cost Time]:    2879.41 secs [ETA]:   11928.99 secs\n",
      "[GAN Epoch]:    29 [Cost Time]:    2981.94 secs [ETA]:   11824.95 secs\n",
      "[GAN Epoch]:    30 [Cost Time]:    3084.49 secs [ETA]:   11721.06 secs\n",
      "[GAN Epoch]:    31 [Cost Time]:    3187.21 secs [ETA]:   11617.91 secs\n",
      "[GAN Epoch]:    32 [Cost Time]:    3289.69 secs [ETA]:   11513.90 secs\n",
      "[GAN Epoch]:    33 [Cost Time]:    3391.05 secs [ETA]:   11406.27 secs\n",
      "[GAN Epoch]:    34 [Cost Time]:    3493.21 secs [ETA]:   11301.58 secs\n",
      "[GAN Epoch]:    35 [Cost Time]:    3597.11 secs [ETA]:   11202.43 secs\n",
      "[GAN Epoch]:    36 [Cost Time]:    3700.70 secs [ETA]:   11102.11 secs\n",
      "[GAN Epoch]:    37 [Cost Time]:    3804.35 secs [ETA]:   11001.76 secs\n",
      "[GAN Epoch]:    38 [Cost Time]:    3907.50 secs [ETA]:   10899.87 secs\n",
      "[GAN Epoch]:    39 [Cost Time]:    4010.94 secs [ETA]:   10798.67 secs\n",
      "[GAN Epoch]:    40 [Cost Time]:    4114.71 secs [ETA]:   10698.24 secs\n",
      "[GAN Epoch]:    41 [Cost Time]:    4218.31 secs [ETA]:   10597.21 secs\n",
      "[GAN Epoch]:    42 [Cost Time]:    4322.19 secs [ETA]:   10496.75 secs\n",
      "[GAN Epoch]:    43 [Cost Time]:    4426.02 secs [ETA]:   10396.01 secs\n",
      "[GAN Epoch]:    44 [Cost Time]:    4529.21 secs [ETA]:   10293.66 secs\n",
      "[GAN Epoch]:    45 [Cost Time]:    4633.04 secs [ETA]:   10192.69 secs\n",
      "[GAN Epoch]:    46 [Cost Time]:    4736.47 secs [ETA]:   10090.73 secs\n",
      "[GAN Epoch]:    47 [Cost Time]:    4840.07 secs [ETA]:    9989.07 secs\n",
      "[GAN Epoch]:    48 [Cost Time]:    4944.28 secs [ETA]:    9888.55 secs\n",
      "[GAN Epoch]:    49 [Cost Time]:    5049.01 secs [ETA]:    9788.90 secs\n",
      "[GAN Epoch]:    50 [Cost Time]:    5153.67 secs [ETA]:    9688.89 secs\n",
      "[GAN Epoch]:    51 [Cost Time]:    5260.50 secs [ETA]:    9592.67 secs\n",
      "[GAN Epoch]:    52 [Cost Time]:    5365.83 secs [ETA]:    9493.40 secs\n",
      "[GAN Epoch]:    53 [Cost Time]:    5471.58 secs [ETA]:    9394.60 secs\n",
      "[GAN Epoch]:    54 [Cost Time]:    5577.23 secs [ETA]:    9295.39 secs\n",
      "[GAN Epoch]:    55 [Cost Time]:    5684.92 secs [ETA]:    9199.23 secs\n",
      "[GAN Epoch]:    56 [Cost Time]:    5792.51 secs [ETA]:    9102.51 secs\n",
      "[GAN Epoch]:    57 [Cost Time]:    5899.74 secs [ETA]:    9004.86 secs\n",
      "[GAN Epoch]:    58 [Cost Time]:    6007.68 secs [ETA]:    8907.94 secs\n",
      "[GAN Epoch]:    59 [Cost Time]:    6109.70 secs [ETA]:    8802.10 secs\n",
      "[GAN Epoch]:    60 [Cost Time]:    6207.42 secs [ETA]:    8690.39 secs\n",
      "[GAN Epoch]:    61 [Cost Time]:    6305.55 secs [ETA]:    8579.68 secs\n",
      "[GAN Epoch]:    62 [Cost Time]:    6403.32 secs [ETA]:    8468.90 secs\n",
      "[GAN Epoch]:    63 [Cost Time]:    6501.70 secs [ETA]:    8359.32 secs\n",
      "[GAN Epoch]:    64 [Cost Time]:    6599.61 secs [ETA]:    8249.51 secs\n",
      "[GAN Epoch]:    65 [Cost Time]:    6697.68 secs [ETA]:    8140.26 secs\n",
      "[GAN Epoch]:    66 [Cost Time]:    6795.83 secs [ETA]:    8031.43 secs\n",
      "[GAN Epoch]:    67 [Cost Time]:    6894.02 secs [ETA]:    7922.98 secs\n",
      "[GAN Epoch]:    68 [Cost Time]:    6992.49 secs [ETA]:    7815.14 secs\n",
      "[GAN Epoch]:    69 [Cost Time]:    7091.11 secs [ETA]:    7707.73 secs\n",
      "[GAN Epoch]:    70 [Cost Time]:    7189.49 secs [ETA]:    7600.32 secs\n",
      "[GAN Epoch]:    71 [Cost Time]:    7287.50 secs [ETA]:    7492.78 secs\n",
      "[GAN Epoch]:    72 [Cost Time]:    7385.66 secs [ETA]:    7385.66 secs\n",
      "[GAN Epoch]:    73 [Cost Time]:    7484.10 secs [ETA]:    7279.06 secs\n",
      "[GAN Epoch]:    74 [Cost Time]:    7582.02 secs [ETA]:    7172.18 secs\n",
      "[GAN Epoch]:    75 [Cost Time]:    7680.04 secs [ETA]:    7065.63 secs\n",
      "[GAN Epoch]:    76 [Cost Time]:    7777.92 secs [ETA]:    6959.19 secs\n",
      "[GAN Epoch]:    77 [Cost Time]:    7876.64 secs [ETA]:    6853.70 secs\n",
      "[GAN Epoch]:    78 [Cost Time]:    7974.59 secs [ETA]:    6747.73 secs\n",
      "[GAN Epoch]:    79 [Cost Time]:    8072.64 secs [ETA]:    6642.05 secs\n",
      "[GAN Epoch]:    80 [Cost Time]:    8170.85 secs [ETA]:    6536.68 secs\n",
      "[GAN Epoch]:    81 [Cost Time]:    8269.40 secs [ETA]:    6431.76 secs\n",
      "[GAN Epoch]:    82 [Cost Time]:    8367.93 secs [ETA]:    6326.97 secs\n",
      "[GAN Epoch]:    83 [Cost Time]:    8466.46 secs [ETA]:    6222.34 secs\n",
      "[GAN Epoch]:    84 [Cost Time]:    8564.66 secs [ETA]:    6117.62 secs\n",
      "[GAN Epoch]:    85 [Cost Time]:    8663.42 secs [ETA]:    6013.43 secs\n",
      "[GAN Epoch]:    86 [Cost Time]:    8761.78 secs [ETA]:    5909.11 secs\n",
      "[GAN Epoch]:    87 [Cost Time]:    8860.54 secs [ETA]:    5805.18 secs\n",
      "[GAN Epoch]:    88 [Cost Time]:    8958.74 secs [ETA]:    5701.02 secs\n",
      "[GAN Epoch]:    89 [Cost Time]:    9057.17 secs [ETA]:    5597.13 secs\n",
      "[GAN Epoch]:    90 [Cost Time]:    9155.24 secs [ETA]:    5493.14 secs\n",
      "[GAN Epoch]:    91 [Cost Time]:    9253.23 secs [ETA]:    5389.25 secs\n",
      "[GAN Epoch]:    92 [Cost Time]:    9351.15 secs [ETA]:    5285.43 secs\n",
      "[GAN Epoch]:    93 [Cost Time]:    9449.20 secs [ETA]:    5181.82 secs\n",
      "[GAN Epoch]:    94 [Cost Time]:    9546.93 secs [ETA]:    5078.15 secs\n",
      "[GAN Epoch]:    95 [Cost Time]:    9645.15 secs [ETA]:    4974.87 secs\n",
      "[GAN Epoch]:    96 [Cost Time]:    9742.67 secs [ETA]:    4871.34 secs\n",
      "[GAN Epoch]:    97 [Cost Time]:    9841.37 secs [ETA]:    4768.50 secs\n",
      "[GAN Epoch]:    98 [Cost Time]:    9940.98 secs [ETA]:    4666.17 secs\n",
      "[GAN Epoch]:    99 [Cost Time]:   10040.62 secs [ETA]:    4563.92 secs\n",
      "[GAN Epoch]:   100 [Cost Time]:   10141.82 secs [ETA]:    4462.40 secs\n",
      "[GAN Epoch]:   101 [Cost Time]:   10243.99 secs [ETA]:    4361.30 secs\n",
      "[GAN Epoch]:   102 [Cost Time]:   10350.11 secs [ETA]:    4261.81 secs\n",
      "[GAN Epoch]:   103 [Cost Time]:   10457.54 secs [ETA]:    4162.71 secs\n",
      "[GAN Epoch]:   104 [Cost Time]:   10566.41 secs [ETA]:    4064.00 secs\n",
      "[GAN Epoch]:   105 [Cost Time]:   10674.21 secs [ETA]:    3964.71 secs\n",
      "[GAN Epoch]:   106 [Cost Time]:   10781.69 secs [ETA]:    3865.13 secs\n",
      "[GAN Epoch]:   107 [Cost Time]:   10889.39 secs [ETA]:    3765.49 secs\n",
      "[GAN Epoch]:   108 [Cost Time]:   10998.18 secs [ETA]:    3666.06 secs\n",
      "[GAN Epoch]:   109 [Cost Time]:   11105.84 secs [ETA]:    3566.10 secs\n",
      "[GAN Epoch]:   110 [Cost Time]:   11212.70 secs [ETA]:    3465.74 secs\n",
      "[GAN Epoch]:   111 [Cost Time]:   11319.31 secs [ETA]:    3365.20 secs\n",
      "[GAN Epoch]:   112 [Cost Time]:   11424.41 secs [ETA]:    3264.12 secs\n",
      "[GAN Epoch]:   113 [Cost Time]:   11529.73 secs [ETA]:    3163.02 secs\n",
      "[GAN Epoch]:   114 [Cost Time]:   11635.72 secs [ETA]:    3062.03 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch]:   115 [Cost Time]:   11741.95 secs [ETA]:    2961.01 secs\n",
      "[GAN Epoch]:   116 [Cost Time]:   11847.13 secs [ETA]:    2859.65 secs\n",
      "[GAN Epoch]:   117 [Cost Time]:   11950.57 secs [ETA]:    2757.82 secs\n",
      "[GAN Epoch]:   118 [Cost Time]:   12054.08 secs [ETA]:    2655.98 secs\n",
      "[GAN Epoch]:   119 [Cost Time]:   12156.93 secs [ETA]:    2553.98 secs\n",
      "[GAN Epoch]:   120 [Cost Time]:   12259.34 secs [ETA]:    2451.87 secs\n",
      "[GAN Epoch]:   121 [Cost Time]:   12357.98 secs [ETA]:    2349.04 secs\n",
      "[GAN Epoch]:   122 [Cost Time]:   12456.31 secs [ETA]:    2246.22 secs\n",
      "[GAN Epoch]:   123 [Cost Time]:   12554.43 secs [ETA]:    2143.44 secs\n",
      "[GAN Epoch]:   124 [Cost Time]:   12652.77 secs [ETA]:    2040.77 secs\n",
      "[GAN Epoch]:   125 [Cost Time]:   12751.06 secs [ETA]:    1938.16 secs\n",
      "[GAN Epoch]:   126 [Cost Time]:   12849.99 secs [ETA]:    1835.71 secs\n",
      "[GAN Epoch]:   127 [Cost Time]:   12949.10 secs [ETA]:    1733.34 secs\n",
      "[GAN Epoch]:   128 [Cost Time]:   13048.16 secs [ETA]:    1631.02 secs\n",
      "[GAN Epoch]:   129 [Cost Time]:   13146.67 secs [ETA]:    1528.68 secs\n",
      "[GAN Epoch]:   130 [Cost Time]:   13245.89 secs [ETA]:    1426.48 secs\n",
      "[GAN Epoch]:   131 [Cost Time]:   13344.73 secs [ETA]:    1324.29 secs\n",
      "[GAN Epoch]:   132 [Cost Time]:   13443.48 secs [ETA]:    1222.13 secs\n",
      "[GAN Epoch]:   133 [Cost Time]:   13543.12 secs [ETA]:    1120.11 secs\n",
      "[GAN Epoch]:   134 [Cost Time]:   13645.17 secs [ETA]:    1018.30 secs\n",
      "[GAN Epoch]:   135 [Cost Time]:   13746.47 secs [ETA]:     916.43 secs\n",
      "[GAN Epoch]:   136 [Cost Time]:   13847.94 secs [ETA]:     814.58 secs\n",
      "[GAN Epoch]:   137 [Cost Time]:   13949.27 secs [ETA]:     712.74 secs\n",
      "[GAN Epoch]:   138 [Cost Time]:   14050.46 secs [ETA]:     610.89 secs\n",
      "[GAN Epoch]:   139 [Cost Time]:   14151.61 secs [ETA]:     509.05 secs\n",
      "[GAN Epoch]:   140 [Cost Time]:   14253.14 secs [ETA]:     407.23 secs\n",
      "[GAN Epoch]:   141 [Cost Time]:   14354.37 secs [ETA]:     305.41 secs\n",
      "[GAN Epoch]:   142 [Cost Time]:   14455.47 secs [ETA]:     203.60 secs\n",
      "[GAN Epoch]:   143 [Cost Time]:   14556.68 secs [ETA]:     101.79 secs\n",
      "[GAN Epoch]:   144 [Cost Time]:   14658.12 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "print('#########################################################################')\n",
    "print('Start Adversarial Training...')\n",
    "log.write('adversarial training...\\n')\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "for total_batch in range(TOTAL_BATCH):\n",
    "    # Train the generator for one step\n",
    "    for it in range(1):\n",
    "        samples = generator.generate(sess)\n",
    "        rewards = rollout.get_reward(sess, samples, 16, discriminator)\n",
    "        feed = {generator.x: samples, generator.rewards: rewards}\n",
    "        _ = sess.run(generator.g_updates, feed_dict=feed)\n",
    "\n",
    "    # Test\n",
    "    '''if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
    "        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
    "        likelihood_data_loader.create_batches(eval_file)\n",
    "        #print(total_batch)        \n",
    "        test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
    "        buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "        print('total_batch: ', total_batch, 'test_loss: ', test_loss)\n",
    "        log.write(buffer)'''\n",
    "\n",
    "    # Update roll-out parameters\n",
    "    rollout.update_params()\n",
    "\n",
    "    # Train the discriminator\n",
    "    for _ in range(3): #default value=range(5)\n",
    "        adversarial_D = ( negative_file + \"adversarial_gen/generator_digit_{0}.txt\").format(str(total_batch+1).zfill(3))\n",
    "        generate_samples(sess, generator, BATCH_SIZE, generated_num, adversarial_D)\n",
    "        dis_data_loader.load_train_data(positive_file, adversarial_D)\n",
    "        \n",
    "        for _ in range(1): #default value=range(3)\n",
    "            dis_data_loader.reset_pointer()\n",
    "            for it in range(dis_data_loader.num_batch):\n",
    "                if it % int(dis_data_loader.num_batch / 30) == 0:\n",
    "                    print(\"Discriminator Epoch {0:5} iteration: {1:6} / {2:6}\".format( (total_batch+1), (it+1), (dis_data_loader.num_batch) ), end=\"\\r\")\n",
    "                x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                feed = {\n",
    "                    discriminator.input_x: x_batch,\n",
    "                    discriminator.input_y: y_batch,\n",
    "                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "                }\n",
    "                _ = sess.run(discriminator.train_op, feed)\n",
    "                \n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (total_batch+1) )*(TOTAL_BATCH-(total_batch+1))\n",
    "    print(\"[GAN Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (total_batch+1), after_time, eta_time))\n",
    "    \n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
