{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "from dataloader import Gen_Data_loader, Dis_dataloader\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from rollout import ROLLOUT\n",
    "#from target_lstm import TARGET_LSTM\n",
    "#import domain_translate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Generator  Hyper-parameters\n",
    "######################################################################################\n",
    "EMB_DIM = 32 # embedding dimension\n",
    "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
    "SEQ_LENGTH = 32 # sequence length\n",
    "START_TOKEN = 0\n",
    "PRE_EPOCH_NUM = 100 # supervise (maximum likelihood estimation) epochs (預設為120)\n",
    "SEED = 88\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Discriminator  Hyper-parameters\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "PRETRAIN_DIS_NUM = 50 #pre-train discriminator times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Basic Training Parameters\n",
    "#########################################################################################\n",
    "TOTAL_BATCH = 144\n",
    "positive_file = ('../Dataset/AlexaTop100K_Separated_Digital/'\n",
    "                  +'TopDomainName.Less33.Separated.Digital-ALL-OF-100000.txt') # 預設值為 'save/real_data.txt'\n",
    "negative_file = 'save/' #預設值為 'save/generator_sample.txt' 更改為會更變\n",
    "eval_file = 'save/eval_file.txt'\n",
    "generated_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n",
    "    Digit2Real = False\n",
    "    # Generate Samples\n",
    "    generated_samples = []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        generated_samples.extend(trainable_model.generate(sess))\n",
    "\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for poem in generated_samples:\n",
    "            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n",
    "            fout.write(buffer)\n",
    "            \n",
    "    if \"adversarial_gen\" in output_file:\n",
    "        now=output_file[37:]\n",
    "        output_file=\"save/adversarial_gen/generator_fake_domain_name_\"+now\n",
    "        Digit2Real = True\n",
    "    \n",
    "    if Digit2Real == True :\n",
    "        digital_table = [\" \",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\".\",\"-\",\n",
    "                            \"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\n",
    "                            \"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\"_\"]    \n",
    "        with open(output_file, 'w') as fout:\n",
    "            for poem in generated_samples:\n",
    "                buffer = \"\".join([digital_table[int(x)] for x in poem]) + '\\n'\n",
    "                fout.write(buffer)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_loss(sess, target_lstm, data_loader):\n",
    "    # target_loss means the oracle negative log-likelihood tested with the oracle model \"target_lstm\"\n",
    "    # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473\n",
    "    nll = []\n",
    "    data_loader.reset_pointer()\n",
    "\n",
    "    for it in range(data_loader.num_batch):\n",
    "        batch = data_loader.next_batch()\n",
    "        #print(target_lstm.pretrain_loss)\n",
    "        #print({target_lstm.x: batch})\n",
    "        g_loss = sess.run(target_lstm.pretrain_loss, {target_lstm.x: batch})\n",
    "        nll.append(g_loss)\n",
    "\n",
    "    return np.mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_train_epoch(sess, trainable_model, data_loader):\n",
    "    # Pre-train the generator using MLE for one epoch\n",
    "    supervised_g_losses = []\n",
    "    data_loader.reset_pointer()\n",
    "    \n",
    "    for it in range(data_loader.num_batch):\n",
    "    #for it in range(50):\n",
    "        #print(\"pre_train\"+str(it))\n",
    "        if it % int(data_loader.num_batch / 30) == 0:\n",
    "            print(\"pre_train_iteration : {0:6} / {1:6}\".format( (it+1), (data_loader.num_batch) ), end=\"\\r\")\n",
    "        batch = data_loader.next_batch()\n",
    "        _, g_loss = trainable_model.pretrain_step(sess, batch)\n",
    "        supervised_g_losses.append(g_loss)\n",
    "\n",
    "    return np.mean(supervised_g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "assert START_TOKEN == 0\n",
    "\n",
    "gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n",
    "likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n",
    "vocab_size = 40 #預設5000 \n",
    "dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
    "\n",
    "generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n",
    "#target_params = pickle.load(open('save/target_params_py3.pkl','rb'))\n",
    "#target_lstm = TARGET_LSTM(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, target_params) # The oracle model\n",
    "\n",
    "discriminator = Discriminator(sequence_length=32, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
    "                            filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 參數配置\n",
    "config = tf.ConfigProto()\n",
    "# 使用allow_growth option，剛一開始分配少量的GPU容量，然後按需慢慢的增加\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pre-training...\n",
      "[MLE Epoch]:     1 [Cost Time]:      76.18 secs [ETA]:    7541.62 secs\n",
      "[MLE Epoch]:     2 [Cost Time]:     146.71 secs [ETA]:    7188.98 secs\n",
      "[MLE Epoch]:     3 [Cost Time]:     217.33 secs [ETA]:    7027.03 secs\n",
      "[MLE Epoch]:     4 [Cost Time]:     290.03 secs [ETA]:    6960.68 secs\n",
      "[MLE Epoch]:     5 [Cost Time]:     362.23 secs [ETA]:    6882.35 secs\n",
      "[MLE Epoch]:     6 [Cost Time]:     439.09 secs [ETA]:    6879.15 secs\n",
      "[MLE Epoch]:     7 [Cost Time]:     510.79 secs [ETA]:    6786.15 secs\n",
      "[MLE Epoch]:     8 [Cost Time]:     583.28 secs [ETA]:    6707.77 secs\n",
      "[MLE Epoch]:     9 [Cost Time]:     656.84 secs [ETA]:    6641.40 secs\n",
      "[MLE Epoch]:    10 [Cost Time]:     728.71 secs [ETA]:    6558.43 secs\n",
      "[MLE Epoch]:    11 [Cost Time]:     805.78 secs [ETA]:    6519.46 secs\n",
      "[MLE Epoch]:    12 [Cost Time]:     877.55 secs [ETA]:    6435.33 secs\n",
      "[MLE Epoch]:    13 [Cost Time]:     949.82 secs [ETA]:    6356.51 secs\n",
      "[MLE Epoch]:    14 [Cost Time]:    1021.91 secs [ETA]:    6277.42 secs\n",
      "[MLE Epoch]:    15 [Cost Time]:    1093.76 secs [ETA]:    6197.95 secs\n",
      "[MLE Epoch]:    16 [Cost Time]:    1170.50 secs [ETA]:    6145.15 secs\n",
      "[MLE Epoch]:    17 [Cost Time]:    1244.67 secs [ETA]:    6076.91 secs\n",
      "[MLE Epoch]:    18 [Cost Time]:    1317.39 secs [ETA]:    6001.43 secs\n",
      "[MLE Epoch]:    19 [Cost Time]:    1388.33 secs [ETA]:    5918.68 secs\n",
      "[MLE Epoch]:    20 [Cost Time]:    1459.51 secs [ETA]:    5838.04 secs\n",
      "[MLE Epoch]:    21 [Cost Time]:    1533.85 secs [ETA]:    5770.19 secs\n",
      "[MLE Epoch]:    22 [Cost Time]:    1604.11 secs [ETA]:    5687.31 secs\n",
      "[MLE Epoch]:    23 [Cost Time]:    1674.27 secs [ETA]:    5605.17 secs\n",
      "[MLE Epoch]:    24 [Cost Time]:    1744.28 secs [ETA]:    5523.55 secs\n",
      "[MLE Epoch]:    25 [Cost Time]:    1814.30 secs [ETA]:    5442.89 secs\n",
      "[MLE Epoch]:    26 [Cost Time]:    1890.03 secs [ETA]:    5379.31 secs\n",
      "[MLE Epoch]:    27 [Cost Time]:    1960.52 secs [ETA]:    5300.67 secs\n",
      "[MLE Epoch]:    28 [Cost Time]:    2031.58 secs [ETA]:    5224.06 secs\n",
      "[MLE Epoch]:    29 [Cost Time]:    2102.68 secs [ETA]:    5147.93 secs\n",
      "[MLE Epoch]:    30 [Cost Time]:    2174.19 secs [ETA]:    5073.11 secs\n",
      "[MLE Epoch]:    31 [Cost Time]:    2259.08 secs [ETA]:    5028.28 secs\n",
      "[MLE Epoch]:    32 [Cost Time]:    2335.69 secs [ETA]:    4963.34 secs\n",
      "[MLE Epoch]:    33 [Cost Time]:    2406.73 secs [ETA]:    4886.39 secs\n",
      "[MLE Epoch]:    34 [Cost Time]:    2478.99 secs [ETA]:    4812.17 secs\n",
      "[MLE Epoch]:    35 [Cost Time]:    2550.02 secs [ETA]:    4735.75 secs\n",
      "[MLE Epoch]:    36 [Cost Time]:    2624.25 secs [ETA]:    4665.33 secs\n",
      "[MLE Epoch]:    37 [Cost Time]:    2695.19 secs [ETA]:    4589.10 secs\n",
      "[MLE Epoch]:    38 [Cost Time]:    2766.62 secs [ETA]:    4513.97 secs\n",
      "[MLE Epoch]:    39 [Cost Time]:    2838.20 secs [ETA]:    4439.24 secs\n",
      "[MLE Epoch]:    40 [Cost Time]:    2909.78 secs [ETA]:    4364.67 secs\n",
      "[MLE Epoch]:    41 [Cost Time]:    2986.33 secs [ETA]:    4297.40 secs\n",
      "[MLE Epoch]:    42 [Cost Time]:    3058.03 secs [ETA]:    4223.00 secs\n",
      "[MLE Epoch]:    43 [Cost Time]:    3129.17 secs [ETA]:    4147.97 secs\n",
      "[MLE Epoch]:    44 [Cost Time]:    3199.49 secs [ETA]:    4072.07 secs\n",
      "[MLE Epoch]:    45 [Cost Time]:    3270.20 secs [ETA]:    3996.91 secs\n",
      "[MLE Epoch]:    46 [Cost Time]:    3344.98 secs [ETA]:    3926.72 secs\n",
      "[MLE Epoch]:    47 [Cost Time]:    3415.86 secs [ETA]:    3851.92 secs\n",
      "[MLE Epoch]:    48 [Cost Time]:    3486.65 secs [ETA]:    3777.20 secs\n",
      "[MLE Epoch]:    49 [Cost Time]:    3556.44 secs [ETA]:    3701.60 secs\n",
      "[MLE Epoch]:    50 [Cost Time]:    3627.34 secs [ETA]:    3627.34 secs\n",
      "[MLE Epoch]:    51 [Cost Time]:    3701.84 secs [ETA]:    3556.67 secs\n",
      "[MLE Epoch]:    52 [Cost Time]:    3771.70 secs [ETA]:    3481.57 secs\n",
      "[MLE Epoch]:    53 [Cost Time]:    3841.25 secs [ETA]:    3406.39 secs\n",
      "[MLE Epoch]:    54 [Cost Time]:    3910.65 secs [ETA]:    3331.29 secs\n",
      "[MLE Epoch]:    55 [Cost Time]:    3981.40 secs [ETA]:    3257.51 secs\n",
      "[MLE Epoch]:    56 [Cost Time]:    4055.86 secs [ETA]:    3186.74 secs\n",
      "[MLE Epoch]:    57 [Cost Time]:    4125.95 secs [ETA]:    3112.56 secs\n",
      "[MLE Epoch]:    58 [Cost Time]:    4195.89 secs [ETA]:    3038.40 secs\n",
      "[MLE Epoch]:    59 [Cost Time]:    4268.02 secs [ETA]:    2965.91 secs\n",
      "[MLE Epoch]:    60 [Cost Time]:    4338.02 secs [ETA]:    2892.01 secs\n",
      "[MLE Epoch]:    61 [Cost Time]:    4412.66 secs [ETA]:    2821.21 secs\n",
      "[MLE Epoch]:    62 [Cost Time]:    4482.59 secs [ETA]:    2747.39 secs\n",
      "[MLE Epoch]:    63 [Cost Time]:    4553.05 secs [ETA]:    2674.01 secs\n",
      "[MLE Epoch]:    64 [Cost Time]:    4623.55 secs [ETA]:    2600.75 secs\n",
      "[MLE Epoch]:    65 [Cost Time]:    4694.22 secs [ETA]:    2527.66 secs\n",
      "[MLE Epoch]:    66 [Cost Time]:    4768.81 secs [ETA]:    2456.66 secs\n",
      "[MLE Epoch]:    67 [Cost Time]:    4839.46 secs [ETA]:    2383.62 secs\n",
      "[MLE Epoch]:    68 [Cost Time]:    4909.10 secs [ETA]:    2310.17 secs\n",
      "[MLE Epoch]:    69 [Cost Time]:    4979.33 secs [ETA]:    2237.09 secs\n",
      "[MLE Epoch]:    70 [Cost Time]:    5049.91 secs [ETA]:    2164.25 secs\n",
      "[MLE Epoch]:    71 [Cost Time]:    5124.15 secs [ETA]:    2092.96 secs\n",
      "[MLE Epoch]:    72 [Cost Time]:    5194.14 secs [ETA]:    2019.94 secs\n",
      "[MLE Epoch]:    73 [Cost Time]:    5264.16 secs [ETA]:    1947.02 secs\n",
      "[MLE Epoch]:    74 [Cost Time]:    5334.48 secs [ETA]:    1874.28 secs\n",
      "[MLE Epoch]:    75 [Cost Time]:    5404.69 secs [ETA]:    1801.56 secs\n",
      "[MLE Epoch]:    76 [Cost Time]:    5479.45 secs [ETA]:    1730.35 secs\n",
      "[MLE Epoch]:    77 [Cost Time]:    5550.25 secs [ETA]:    1657.87 secs\n",
      "[MLE Epoch]:    78 [Cost Time]:    5620.28 secs [ETA]:    1585.21 secs\n",
      "[MLE Epoch]:    79 [Cost Time]:    5690.41 secs [ETA]:    1512.64 secs\n",
      "[MLE Epoch]:    80 [Cost Time]:    5760.62 secs [ETA]:    1440.16 secs\n",
      "[MLE Epoch]:    81 [Cost Time]:    5835.39 secs [ETA]:    1368.79 secs\n",
      "[MLE Epoch]:    82 [Cost Time]:    5907.08 secs [ETA]:    1296.68 secs\n",
      "[MLE Epoch]:    83 [Cost Time]:    5979.62 secs [ETA]:    1224.74 secs\n",
      "[MLE Epoch]:    84 [Cost Time]:    6056.29 secs [ETA]:    1153.58 secs\n",
      "[MLE Epoch]:    85 [Cost Time]:    6130.40 secs [ETA]:    1081.84 secs\n",
      "[MLE Epoch]:    86 [Cost Time]:    6206.66 secs [ETA]:    1010.39 secs\n",
      "[MLE Epoch]:    87 [Cost Time]:    6277.94 secs [ETA]:     938.08 secs\n",
      "[MLE Epoch]:    88 [Cost Time]:    6348.40 secs [ETA]:     865.69 secs\n",
      "[MLE Epoch]:    89 [Cost Time]:    6418.60 secs [ETA]:     793.31 secs\n",
      "[MLE Epoch]:    90 [Cost Time]:    6490.93 secs [ETA]:     721.21 secs\n",
      "[MLE Epoch]:    91 [Cost Time]:    6567.75 secs [ETA]:     649.56 secs\n",
      "[MLE Epoch]:    92 [Cost Time]:    6641.92 secs [ETA]:     577.56 secs\n",
      "[MLE Epoch]:    93 [Cost Time]:    6716.35 secs [ETA]:     505.53 secs\n",
      "[MLE Epoch]:    94 [Cost Time]:    6789.85 secs [ETA]:     433.39 secs\n",
      "[MLE Epoch]:    95 [Cost Time]:    6865.51 secs [ETA]:     361.34 secs\n",
      "[MLE Epoch]:    96 [Cost Time]:    6946.96 secs [ETA]:     289.46 secs\n",
      "[MLE Epoch]:    97 [Cost Time]:    7022.03 secs [ETA]:     217.18 secs\n",
      "[MLE Epoch]:    98 [Cost Time]:    7093.40 secs [ETA]:     144.76 secs\n",
      "[MLE Epoch]:    99 [Cost Time]:    7164.22 secs [ETA]:      72.37 secs\n",
      "[MLE Epoch]:   100 [Cost Time]:    7234.53 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "# First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n",
    "#generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file)\n",
    "gen_data_loader.create_batches(positive_file)\n",
    "\n",
    "log = open('save/experiment-log.txt', 'w')\n",
    "#  pre-train generator\n",
    "print('Start pre-training...')\n",
    "log.write('pre-training...\\n')\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "for epoch in range(PRE_EPOCH_NUM):\n",
    "    loss = pre_train_epoch(sess, generator, gen_data_loader)    \n",
    "    if epoch % 5 == 0:\n",
    "        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
    "        likelihood_data_loader.create_batches(eval_file)\n",
    "        '''test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
    "        print('pre-train epoch ', epoch, 'test_loss ', test_loss)\n",
    "        buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "        log.write(buffer)'''\n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (epoch+1) )*(PRE_EPOCH_NUM-(epoch+1))\n",
    "    print(\"[MLE Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (epoch+1), after_time, eta_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pre-training discriminator...\n",
      "[Pre-train Discriminator Epoch]:     1 [Cost Time]:      88.91 secs [ETA]:    4356.59 secs\n",
      "[Pre-train Discriminator Epoch]:     2 [Cost Time]:     167.21 secs [ETA]:    4013.15 secs\n",
      "[Pre-train Discriminator Epoch]:     3 [Cost Time]:     245.20 secs [ETA]:    3841.50 secs\n",
      "[Pre-train Discriminator Epoch]:     4 [Cost Time]:     323.23 secs [ETA]:    3717.11 secs\n",
      "[Pre-train Discriminator Epoch]:     5 [Cost Time]:     402.13 secs [ETA]:    3619.19 secs\n",
      "[Pre-train Discriminator Epoch]:     6 [Cost Time]:     480.36 secs [ETA]:    3522.65 secs\n",
      "[Pre-train Discriminator Epoch]:     7 [Cost Time]:     559.66 secs [ETA]:    3437.93 secs\n",
      "[Pre-train Discriminator Epoch]:     8 [Cost Time]:     639.58 secs [ETA]:    3357.80 secs\n",
      "[Pre-train Discriminator Epoch]:     9 [Cost Time]:     718.02 secs [ETA]:    3270.98 secs\n",
      "[Pre-train Discriminator Epoch]:    10 [Cost Time]:     796.40 secs [ETA]:    3185.58 secs\n",
      "[Pre-train Discriminator Epoch]:    11 [Cost Time]:     874.92 secs [ETA]:    3101.99 secs\n",
      "[Pre-train Discriminator Epoch]:    12 [Cost Time]:     953.89 secs [ETA]:    3020.64 secs\n",
      "[Pre-train Discriminator Epoch]:    13 [Cost Time]:    1032.10 secs [ETA]:    2937.50 secs\n",
      "[Pre-train Discriminator Epoch]:    14 [Cost Time]:    1111.25 secs [ETA]:    2857.50 secs\n",
      "[Pre-train Discriminator Epoch]:    15 [Cost Time]:    1190.27 secs [ETA]:    2777.29 secs\n",
      "[Pre-train Discriminator Epoch]:    16 [Cost Time]:    1270.04 secs [ETA]:    2698.83 secs\n",
      "[Pre-train Discriminator Epoch]:    17 [Cost Time]:    1351.01 secs [ETA]:    2622.54 secs\n",
      "[Pre-train Discriminator Epoch]:    18 [Cost Time]:    1429.65 secs [ETA]:    2541.60 secs\n",
      "[Pre-train Discriminator Epoch]:    19 [Cost Time]:    1512.23 secs [ETA]:    2467.32 secs\n",
      "[Pre-train Discriminator Epoch]:    20 [Cost Time]:    1588.95 secs [ETA]:    2383.42 secs\n",
      "[Pre-train Discriminator Epoch]:    21 [Cost Time]:    1665.33 secs [ETA]:    2299.74 secs\n",
      "[Pre-train Discriminator Epoch]:    22 [Cost Time]:    1741.75 secs [ETA]:    2216.77 secs\n",
      "[Pre-train Discriminator Epoch]:    23 [Cost Time]:    1818.51 secs [ETA]:    2134.77 secs\n",
      "[Pre-train Discriminator Epoch]:    24 [Cost Time]:    1895.00 secs [ETA]:    2052.92 secs\n",
      "[Pre-train Discriminator Epoch]:    25 [Cost Time]:    1971.59 secs [ETA]:    1971.59 secs\n",
      "[Pre-train Discriminator Epoch]:    26 [Cost Time]:    2048.02 secs [ETA]:    1890.48 secs\n",
      "[Pre-train Discriminator Epoch]:    27 [Cost Time]:    2124.43 secs [ETA]:    1809.70 secs\n",
      "[Pre-train Discriminator Epoch]:    28 [Cost Time]:    2201.30 secs [ETA]:    1729.59 secs\n",
      "[Pre-train Discriminator Epoch]:    29 [Cost Time]:    2278.16 secs [ETA]:    1649.70 secs\n",
      "[Pre-train Discriminator Epoch]:    30 [Cost Time]:    2354.58 secs [ETA]:    1569.72 secs\n",
      "[Pre-train Discriminator Epoch]:    31 [Cost Time]:    2431.20 secs [ETA]:    1490.09 secs\n",
      "[Pre-train Discriminator Epoch]:    32 [Cost Time]:    2507.67 secs [ETA]:    1410.57 secs\n",
      "[Pre-train Discriminator Epoch]:    33 [Cost Time]:    2584.96 secs [ETA]:    1331.65 secs\n",
      "[Pre-train Discriminator Epoch]:    34 [Cost Time]:    2661.59 secs [ETA]:    1252.51 secs\n",
      "[Pre-train Discriminator Epoch]:    35 [Cost Time]:    2738.53 secs [ETA]:    1173.66 secs\n",
      "[Pre-train Discriminator Epoch]:    36 [Cost Time]:    2815.62 secs [ETA]:    1094.96 secs\n",
      "[Pre-train Discriminator Epoch]:    37 [Cost Time]:    2892.15 secs [ETA]:    1016.16 secs\n",
      "[Pre-train Discriminator Epoch]:    38 [Cost Time]:    2968.64 secs [ETA]:     937.47 secs\n",
      "[Pre-train Discriminator Epoch]:    39 [Cost Time]:    3045.13 secs [ETA]:     858.88 secs\n",
      "[Pre-train Discriminator Epoch]:    40 [Cost Time]:    3122.26 secs [ETA]:     780.56 secs\n",
      "[Pre-train Discriminator Epoch]:    41 [Cost Time]:    3199.27 secs [ETA]:     702.28 secs\n",
      "[Pre-train Discriminator Epoch]:    42 [Cost Time]:    3275.88 secs [ETA]:     623.98 secs\n",
      "[Pre-train Discriminator Epoch]:    43 [Cost Time]:    3352.41 secs [ETA]:     545.74 secs\n",
      "[Pre-train Discriminator Epoch]:    44 [Cost Time]:    3429.29 secs [ETA]:     467.63 secs\n",
      "[Pre-train Discriminator Epoch]:    45 [Cost Time]:    3506.43 secs [ETA]:     389.60 secs\n",
      "[Pre-train Discriminator Epoch]:    46 [Cost Time]:    3583.46 secs [ETA]:     311.61 secs\n",
      "[Pre-train Discriminator Epoch]:    47 [Cost Time]:    3660.18 secs [ETA]:     233.63 secs\n",
      "[Pre-train Discriminator Epoch]:    48 [Cost Time]:    3736.70 secs [ETA]:     155.70 secs\n",
      "[Pre-train Discriminator Epoch]:    49 [Cost Time]:    3813.69 secs [ETA]:      77.83 secs\n",
      "[Pre-train Discriminator Epoch]:    50 [Cost Time]:    3890.20 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "print('Start pre-training discriminator...')\n",
    "# Train 3 epoch on the generated data and do this for 50 times\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "pre_D_epoch = 0\n",
    "for _ in range(PRETRAIN_DIS_NUM): #50\n",
    "    pretrain_D = ( negative_file + \"pretrain_discriminator.txt\")\n",
    "    generate_samples(sess, generator, BATCH_SIZE, generated_num, pretrain_D)\n",
    "    dis_data_loader.load_train_data(positive_file, pretrain_D)\n",
    "    for _ in range(3):\n",
    "        dis_data_loader.reset_pointer()\n",
    "        for it in range(dis_data_loader.num_batch):\n",
    "            if it % int(dis_data_loader.num_batch / 30) == 0:\n",
    "                print(\"pre_train_iteration : {0:6} / {1:6}\".format( (it+1), (dis_data_loader.num_batch) ), end=\"\\r\")\n",
    "            x_batch, y_batch = dis_data_loader.next_batch()\n",
    "            feed = {\n",
    "                discriminator.input_x: x_batch,\n",
    "                discriminator.input_y: y_batch,\n",
    "                discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "            }\n",
    "            _ = sess.run(discriminator.train_op, feed)\n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (pre_D_epoch+1) )*(PRETRAIN_DIS_NUM-(pre_D_epoch+1))\n",
    "    print(\"[Pre-train Discriminator Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (pre_D_epoch+1), after_time, eta_time))\n",
    "    pre_D_epoch = pre_D_epoch +1\n",
    "\n",
    "rollout = ROLLOUT(generator, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "Start Adversarial Training...\n",
      "[GAN Epoch]:     1 [Cost Time]:     395.20 secs [ETA]:   56513.52 secs\n",
      "[GAN Epoch]:     2 [Cost Time]:     790.21 secs [ETA]:   56105.20 secs\n",
      "[GAN Epoch]:     3 [Cost Time]:    1184.96 secs [ETA]:   55693.23 secs\n",
      "[GAN Epoch]:     4 [Cost Time]:    1580.93 secs [ETA]:   55332.71 secs\n",
      "[GAN Epoch]:     5 [Cost Time]:    1975.07 secs [ETA]:   54906.81 secs\n",
      "[GAN Epoch]:     6 [Cost Time]:    2368.33 secs [ETA]:   54471.68 secs\n",
      "[GAN Epoch]:     7 [Cost Time]:    2761.66 secs [ETA]:   54049.61 secs\n",
      "[GAN Epoch]:     8 [Cost Time]:    3155.21 secs [ETA]:   53638.57 secs\n",
      "[GAN Epoch]:     9 [Cost Time]:    3549.69 secs [ETA]:   53245.28 secs\n",
      "[GAN Epoch]:    10 [Cost Time]:    3943.79 secs [ETA]:   52846.72 secs\n",
      "[GAN Epoch]:    11 [Cost Time]:    4338.91 secs [ETA]:   52461.38 secs\n",
      "[GAN Epoch]:    12 [Cost Time]:    4734.13 secs [ETA]:   52075.42 secs\n",
      "[GAN Epoch]:    13 [Cost Time]:    5129.28 secs [ETA]:   51687.33 secs\n",
      "[GAN Epoch]:    14 [Cost Time]:    5524.12 secs [ETA]:   51295.36 secs\n",
      "[GAN Epoch]:    15 [Cost Time]:    5918.86 secs [ETA]:   50902.16 secs\n",
      "[GAN Epoch]:    16 [Cost Time]:    6314.24 secs [ETA]:   50513.92 secs\n",
      "[GAN Epoch]:    17 [Cost Time]:    6709.12 secs [ETA]:   50121.08 secs\n",
      "[GAN Epoch]:    18 [Cost Time]:    7104.20 secs [ETA]:   49729.41 secs\n",
      "[GAN Epoch]:    19 [Cost Time]:    7499.63 secs [ETA]:   49339.66 secs\n",
      "[GAN Epoch]:    20 [Cost Time]:    7894.96 secs [ETA]:   48948.77 secs\n",
      "[GAN Epoch]:    21 [Cost Time]:    8290.14 secs [ETA]:   48556.53 secs\n",
      "[GAN Epoch]:    22 [Cost Time]:    8686.12 secs [ETA]:   48168.46 secs\n",
      "[GAN Epoch]:    23 [Cost Time]:    9082.66 secs [ETA]:   47782.71 secs\n",
      "[GAN Epoch]:    24 [Cost Time]:    9478.41 secs [ETA]:   47392.06 secs\n",
      "[GAN Epoch]:    25 [Cost Time]:    9874.30 secs [ETA]:   47001.68 secs\n",
      "[GAN Epoch]:    26 [Cost Time]:   10269.82 secs [ETA]:   46609.20 secs\n",
      "[GAN Epoch]:    27 [Cost Time]:   10666.68 secs [ETA]:   46222.29 secs\n",
      "[GAN Epoch]:    28 [Cost Time]:   11063.14 secs [ETA]:   45833.00 secs\n",
      "[GAN Epoch]:    29 [Cost Time]:   11459.68 secs [ETA]:   45443.55 secs\n",
      "[GAN Epoch]:    30 [Cost Time]:   11855.39 secs [ETA]:   45050.49 secs\n",
      "[GAN Epoch]:    31 [Cost Time]:   12251.30 secs [ETA]:   44657.96 secs\n",
      "[GAN Epoch]:    32 [Cost Time]:   12647.80 secs [ETA]:   44267.30 secs\n",
      "[GAN Epoch]:    33 [Cost Time]:   13045.35 secs [ETA]:   43879.83 secs\n",
      "[GAN Epoch]:    34 [Cost Time]:   13441.98 secs [ETA]:   43488.75 secs\n",
      "[GAN Epoch]:    35 [Cost Time]:   13839.51 secs [ETA]:   43100.20 secs\n",
      "[GAN Epoch]:    36 [Cost Time]:   14236.71 secs [ETA]:   42710.12 secs\n",
      "[GAN Epoch]:    37 [Cost Time]:   14634.51 secs [ETA]:   42321.42 secs\n",
      "[GAN Epoch]:    38 [Cost Time]:   15033.99 secs [ETA]:   41936.92 secs\n",
      "[GAN Epoch]:    39 [Cost Time]:   15433.88 secs [ETA]:   41552.75 secs\n",
      "[GAN Epoch]:    40 [Cost Time]:   15838.08 secs [ETA]:   41179.02 secs\n",
      "[GAN Epoch]:    41 [Cost Time]:   16241.23 secs [ETA]:   40801.14 secs\n",
      "[GAN Epoch]:    42 [Cost Time]:   16648.94 secs [ETA]:   40433.13 secs\n",
      "[GAN Epoch]:    43 [Cost Time]:   17062.37 secs [ETA]:   40076.72 secs\n",
      "[GAN Epoch]:    44 [Cost Time]:   17475.52 secs [ETA]:   39717.09 secs\n",
      "[GAN Epoch]:    45 [Cost Time]:   17888.42 secs [ETA]:   39354.53 secs\n",
      "[GAN Epoch]:    46 [Cost Time]:   18287.77 secs [ETA]:   38960.90 secs\n",
      "[GAN Epoch]:    47 [Cost Time]:   18692.33 secs [ETA]:   38577.78 secs\n",
      "[GAN Epoch]:    48 [Cost Time]:   19101.57 secs [ETA]:   38203.14 secs\n",
      "[GAN Epoch]:    49 [Cost Time]:   19505.86 secs [ETA]:   37817.49 secs\n",
      "[GAN Epoch]:    50 [Cost Time]:   19913.18 secs [ETA]:   37436.77 secs\n",
      "[GAN Epoch]:    51 [Cost Time]:   20321.49 secs [ETA]:   37056.84 secs\n",
      "[GAN Epoch]:    52 [Cost Time]:   20733.18 secs [ETA]:   36681.78 secs\n",
      "[GAN Epoch]:    53 [Cost Time]:   21144.24 secs [ETA]:   36304.26 secs\n",
      "[GAN Epoch]:    54 [Cost Time]:   21556.05 secs [ETA]:   35926.75 secs\n",
      "[GAN Epoch]:    55 [Cost Time]:   21967.00 secs [ETA]:   35546.60 secs\n",
      "[GAN Epoch]:    56 [Cost Time]:   22380.09 secs [ETA]:   35168.72 secs\n",
      "[GAN Epoch]:    57 [Cost Time]:   22792.40 secs [ETA]:   34788.40 secs\n",
      "[GAN Epoch]:    58 [Cost Time]:   23201.45 secs [ETA]:   34402.15 secs\n",
      "[GAN Epoch]:    59 [Cost Time]:   23611.50 secs [ETA]:   34016.56 secs\n",
      "[GAN Epoch]:    60 [Cost Time]:   24023.07 secs [ETA]:   33632.30 secs\n",
      "[GAN Epoch]:    61 [Cost Time]:   24434.77 secs [ETA]:   33247.31 secs\n",
      "[GAN Epoch]:    62 [Cost Time]:   24849.99 secs [ETA]:   32866.11 secs\n",
      "[GAN Epoch]:    63 [Cost Time]:   25264.68 secs [ETA]:   32483.15 secs\n",
      "[GAN Epoch]:    64 [Cost Time]:   25677.38 secs [ETA]:   32096.72 secs\n",
      "[GAN Epoch]:    65 [Cost Time]:   26091.95 secs [ETA]:   31711.75 secs\n",
      "[GAN Epoch]:    66 [Cost Time]:   26507.29 secs [ETA]:   31326.80 secs\n",
      "[GAN Epoch]:    67 [Cost Time]:   26921.49 secs [ETA]:   30939.62 secs\n",
      "[GAN Epoch]:    68 [Cost Time]:   27336.60 secs [ETA]:   30552.67 secs\n",
      "[GAN Epoch]:    69 [Cost Time]:   27752.66 secs [ETA]:   30165.93 secs\n",
      "[GAN Epoch]:    70 [Cost Time]:   28169.58 secs [ETA]:   29779.27 secs\n",
      "[GAN Epoch]:    71 [Cost Time]:   28585.62 secs [ETA]:   29390.85 secs\n",
      "[GAN Epoch]:    72 [Cost Time]:   29005.58 secs [ETA]:   29005.58 secs\n",
      "[GAN Epoch]:    73 [Cost Time]:   29425.65 secs [ETA]:   28619.47 secs\n",
      "[GAN Epoch]:    74 [Cost Time]:   29851.44 secs [ETA]:   28237.84 secs\n",
      "[GAN Epoch]:    75 [Cost Time]:   30286.15 secs [ETA]:   27863.25 secs\n",
      "[GAN Epoch]:    76 [Cost Time]:   30724.38 secs [ETA]:   27490.24 secs\n",
      "[GAN Epoch]:    77 [Cost Time]:   31162.07 secs [ETA]:   27115.05 secs\n",
      "[GAN Epoch]:    78 [Cost Time]:   31606.50 secs [ETA]:   26743.96 secs\n",
      "[GAN Epoch]:    79 [Cost Time]:   32033.41 secs [ETA]:   26356.60 secs\n",
      "[GAN Epoch]:    80 [Cost Time]:   32462.80 secs [ETA]:   25970.24 secs\n",
      "[GAN Epoch]:    81 [Cost Time]:   32889.14 secs [ETA]:   25580.44 secs\n",
      "[GAN Epoch]:    82 [Cost Time]:   33302.02 secs [ETA]:   25179.57 secs\n",
      "[GAN Epoch]:    83 [Cost Time]:   33715.64 secs [ETA]:   24778.96 secs\n",
      "[GAN Epoch]:    84 [Cost Time]:   34128.74 secs [ETA]:   24377.67 secs\n",
      "[GAN Epoch]:    85 [Cost Time]:   34543.15 secs [ETA]:   23977.01 secs\n",
      "[GAN Epoch]:    86 [Cost Time]:   34958.39 secs [ETA]:   23576.59 secs\n",
      "[GAN Epoch]:    87 [Cost Time]:   35372.41 secs [ETA]:   23175.03 secs\n",
      "[GAN Epoch]:    88 [Cost Time]:   35786.49 secs [ETA]:   22773.22 secs\n",
      "[GAN Epoch]:    89 [Cost Time]:   36200.46 secs [ETA]:   22371.07 secs\n",
      "[GAN Epoch]:    90 [Cost Time]:   36615.85 secs [ETA]:   21969.51 secs\n",
      "[GAN Epoch]:    91 [Cost Time]:   37031.51 secs [ETA]:   21567.80 secs\n",
      "[GAN Epoch]:    92 [Cost Time]:   37449.88 secs [ETA]:   21167.32 secs\n",
      "[GAN Epoch]:    93 [Cost Time]:   37867.63 secs [ETA]:   20766.12 secs\n",
      "[GAN Epoch]:    94 [Cost Time]:   38286.80 secs [ETA]:   20365.32 secs\n",
      "[GAN Epoch]:    95 [Cost Time]:   38705.95 secs [ETA]:   19964.12 secs\n",
      "[GAN Epoch]:    96 [Cost Time]:   39125.68 secs [ETA]:   19562.84 secs\n",
      "[GAN Epoch]:    97 [Cost Time]:   39545.57 secs [ETA]:   19161.26 secs\n",
      "[GAN Epoch]:    98 [Cost Time]:   39965.87 secs [ETA]:   18759.49 secs\n",
      "[GAN Epoch]:    99 [Cost Time]:   40387.41 secs [ETA]:   18357.91 secs\n",
      "[GAN Epoch]:   100 [Cost Time]:   40811.35 secs [ETA]:   17956.99 secs\n",
      "[GAN Epoch]:   101 [Cost Time]:   41236.05 secs [ETA]:   17555.94 secs\n",
      "[GAN Epoch]:   102 [Cost Time]:   41659.42 secs [ETA]:   17153.88 secs\n",
      "[GAN Epoch]:   103 [Cost Time]:   42082.79 secs [ETA]:   16751.40 secs\n",
      "[GAN Epoch]:   104 [Cost Time]:   42506.40 secs [ETA]:   16348.62 secs\n",
      "[GAN Epoch]:   105 [Cost Time]:   42930.33 secs [ETA]:   15945.55 secs\n",
      "[GAN Epoch]:   106 [Cost Time]:   43355.71 secs [ETA]:   15542.61 secs\n",
      "[GAN Epoch]:   107 [Cost Time]:   43784.07 secs [ETA]:   15140.29 secs\n",
      "[GAN Epoch]:   108 [Cost Time]:   44210.81 secs [ETA]:   14736.94 secs\n",
      "[GAN Epoch]:   109 [Cost Time]:   44638.90 secs [ETA]:   14333.59 secs\n",
      "[GAN Epoch]:   110 [Cost Time]:   45064.98 secs [ETA]:   13929.18 secs\n",
      "[GAN Epoch]:   111 [Cost Time]:   45491.66 secs [ETA]:   13524.55 secs\n",
      "[GAN Epoch]:   112 [Cost Time]:   45918.95 secs [ETA]:   13119.70 secs\n",
      "[GAN Epoch]:   113 [Cost Time]:   46348.26 secs [ETA]:   12715.01 secs\n",
      "[GAN Epoch]:   114 [Cost Time]:   46774.74 secs [ETA]:   12309.14 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch]:   115 [Cost Time]:   47204.49 secs [ETA]:   11903.74 secs\n",
      "[GAN Epoch]:   116 [Cost Time]:   47634.92 secs [ETA]:   11498.08 secs\n",
      "[GAN Epoch]:   117 [Cost Time]:   48065.06 secs [ETA]:   11091.94 secs\n",
      "[GAN Epoch]:   118 [Cost Time]:   48497.65 secs [ETA]:   10685.92 secs\n",
      "[GAN Epoch]:   119 [Cost Time]:   48928.51 secs [ETA]:   10279.10 secs\n",
      "[GAN Epoch]:   120 [Cost Time]:   49361.07 secs [ETA]:    9872.21 secs\n",
      "[GAN Epoch]:   121 [Cost Time]:   49793.27 secs [ETA]:    9464.84 secs\n",
      "[GAN Epoch]:   122 [Cost Time]:   50222.14 secs [ETA]:    9056.45 secs\n",
      "[GAN Epoch]:   123 [Cost Time]:   50656.96 secs [ETA]:    8648.75 secs\n",
      "[GAN Epoch]:   124 [Cost Time]:   51091.47 secs [ETA]:    8240.56 secs\n",
      "[GAN Epoch]:   125 [Cost Time]:   51528.05 secs [ETA]:    7832.26 secs\n",
      "[GAN Epoch]:   126 [Cost Time]:   51961.15 secs [ETA]:    7423.02 secs\n",
      "[GAN Epoch]:   127 [Cost Time]:   52390.44 secs [ETA]:    7012.89 secs\n",
      "[GAN Epoch]:   128 [Cost Time]:   52826.43 secs [ETA]:    6603.30 secs\n",
      "[GAN Epoch]:   129 [Cost Time]:   53261.24 secs [ETA]:    6193.17 secs\n",
      "[GAN Epoch]:   130 [Cost Time]:   53698.89 secs [ETA]:    5782.96 secs\n",
      "[GAN Epoch]:   131 [Cost Time]:   54138.45 secs [ETA]:    5372.52 secs\n",
      "[GAN Epoch]:   132 [Cost Time]:   54576.33 secs [ETA]:    4961.48 secs\n",
      "[GAN Epoch]:   133 [Cost Time]:   55016.43 secs [ETA]:    4550.23 secs\n",
      "[GAN Epoch]:   134 [Cost Time]:   55458.20 secs [ETA]:    4138.67 secs\n",
      "[GAN Epoch]:   135 [Cost Time]:   55900.21 secs [ETA]:    3726.68 secs\n",
      "[GAN Epoch]:   136 [Cost Time]:   56340.71 secs [ETA]:    3314.16 secs\n",
      "[GAN Epoch]:   137 [Cost Time]:   56783.53 secs [ETA]:    2901.35 secs\n",
      "[GAN Epoch]:   138 [Cost Time]:   57227.74 secs [ETA]:    2488.16 secs\n",
      "[GAN Epoch]:   139 [Cost Time]:   57669.89 secs [ETA]:    2074.46 secs\n",
      "[GAN Epoch]:   140 [Cost Time]:   58112.38 secs [ETA]:    1660.35 secs\n",
      "[GAN Epoch]:   141 [Cost Time]:   58555.42 secs [ETA]:    1245.86 secs\n",
      "[GAN Epoch]:   142 [Cost Time]:   59000.81 secs [ETA]:     831.00 secs\n",
      "[GAN Epoch]:   143 [Cost Time]:   59443.79 secs [ETA]:     415.69 secs\n",
      "[GAN Epoch]:   144 [Cost Time]:   59887.53 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "print('#########################################################################')\n",
    "print('Start Adversarial Training...')\n",
    "log.write('adversarial training...\\n')\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "for total_batch in range(TOTAL_BATCH):\n",
    "    # Train the generator for one step\n",
    "    for it in range(1):\n",
    "        samples = generator.generate(sess)\n",
    "        rewards = rollout.get_reward(sess, samples, 16, discriminator)\n",
    "        feed = {generator.x: samples, generator.rewards: rewards}\n",
    "        _ = sess.run(generator.g_updates, feed_dict=feed)\n",
    "\n",
    "    # Test\n",
    "    '''if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
    "        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
    "        likelihood_data_loader.create_batches(eval_file)\n",
    "        #print(total_batch)        \n",
    "        test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
    "        buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "        print('total_batch: ', total_batch, 'test_loss: ', test_loss)\n",
    "        log.write(buffer)'''\n",
    "\n",
    "    # Update roll-out parameters\n",
    "    rollout.update_params()\n",
    "\n",
    "    # Train the discriminator\n",
    "    for _ in range(5):\n",
    "        adversarial_D = ( negative_file + \"adversarial_gen/generator_digit_{0}.txt\").format(str(total_batch+1).zfill(3))\n",
    "        generate_samples(sess, generator, BATCH_SIZE, generated_num, adversarial_D)\n",
    "        dis_data_loader.load_train_data(positive_file, adversarial_D)\n",
    "        \n",
    "        for _ in range(3): #default value=range(3)\n",
    "            dis_data_loader.reset_pointer()\n",
    "            for it in range(dis_data_loader.num_batch):\n",
    "                if it % int(dis_data_loader.num_batch / 30) == 0:\n",
    "                    print(\"Discriminator Epoch {0:5} iteration: {1:6} / {2:6}\".format( (total_batch+1), (it+1), (dis_data_loader.num_batch) ), end=\"\\r\")\n",
    "                x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                feed = {\n",
    "                    discriminator.input_x: x_batch,\n",
    "                    discriminator.input_y: y_batch,\n",
    "                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "                }\n",
    "                _ = sess.run(discriminator.train_op, feed)\n",
    "                \n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (total_batch+1) )*(TOTAL_BATCH-(total_batch+1))\n",
    "    print(\"[GAN Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (total_batch+1), after_time, eta_time))\n",
    "    \n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
